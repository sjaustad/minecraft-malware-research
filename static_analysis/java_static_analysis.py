import re, javalang, gcld3, nltk, os, unicodedata
import string, zipfile
import math
import numpy as np
from collections import Counter
import ipaddress


detector = gcld3.NNetLanguageIdentifier(0,100)

class JavaStaticAnalysis:
    def __init__(self):
        self.results = {}

        # Define the sensitive APIs and keywords
        # Define the list of suspicious API strings for classes
        self.suspicious_apis = [
            "Runtime", "ProcessBuilder", "System", "Desktop", "JShell", "ScriptEngine",
            "Base64$Decoder", "Base64$Encoder", "Socket", "URL", "URI", "URLConnection",
            "HttpRequest$Builder", "URLClassLoader", "ClassLoader", "Class", "Method",
            "Introspector", "System", "InetAddress", "FileOutputStream", "File", "Files",
            "FileWriter", "BufferedWriter", "RandomAccessFile", "FileInputStream", "Files",
            "FileReader", "Scanner", "BufferedReader", "RandomAccessFile"
        ]
        # Define the list of suspicious strings for methods/constructors
        self.suspicious_method_strings = [
            "exec", "ProcessBuilder", "command", "start", "load", "loadLibrary", "open",
            "eval", "decode", "encode", "encodeToString", "Socket", "getInputStream",
            "getOutputStream", "URL", "openConnection", "openStream", "URI", "create",
            "getInputStream", "GET", "POST", "URLClassLoader", "loadClass", "forName",
            "getDeclaredMethod", "getDeclaredField", "newInstance", "invoke", "getBeanInfo",
            "getProperty", "getProperties", "getEnv", "getHostName", "FileOutputStream",
            "write", "File", "newBufferedWriter", "newOutputStream", "write", "writeString",
            "copy", "write", "FileInputStream", "read", "newInputStream", "newBufferedReader",
            "readAllBytes", "readAllLines", "copy", "read", "Scanner", "read", "readFully"
        ]
        self.api_dict = {
            "runtime":['exec'],
            "processbuilder": ["processbuilder", "command", "start"],
            "system":['load','loadlibrary'],
            "desktop": ["open"],
            "jshell": ["eval"],
            "scriptengine": ["eval"],
            "base64$decoder": ["decode"],
            "base64$encoder": ["encode", "encodetostring"],
            "socket": ["socket", "getinputstream", "getoutputstream"],
            "url": ["url", "openconnection", "openstream"],
            "uri": ["uri", "create"],
            "urlconnection": ["getinputstream"],
            "httprequest$builder": ["get", "post"],
            "urlclassloader": ["urlclassloader"],
            "classloader": ["loadclass"],
            "class": ["forname", "getdeclaredmethod", "getdeclaredfield", "newinstance"],
            "method": ["invoke"],
            "introspector": ["getbeaninfo"],
            "system": ["getproperty", "getproperties", "getenv"],
            "inetaddress": ["gethostname"],
            "fileoutputstream": ["fileoutputstream", "write"],
            "file": ["file"],
            "files": ["newbufferedwriter", "newoutputstream", "write", "writestring", "copy"],
            "filewriter": ["write"],
            "bufferedwriter": ["write"],
            "randomaccessfile": ["write"],
            "fileinputstream": ["fileinputstream", "read"],
            "filereader": ["read"],
            "scanner": ["scanner"],
            "bufferedreader": ["read"],
            "randomaccessfile_read": ["read", "readfully"]
        }
        self.sensitive_keywords = [
            "runtime.exec",
            "processbuilder",
            "system.exec",
            "runtime.getruntime().exec",
            "process.start",
            "cmd.exe",
            "powershell.exe",
            "inetaddress",
            "httpurlconnection",
            "httpsurlconnection",
            "datagramsocket",
            "multicastsocket",
            "java.net.url",
            "jshell",
            "scriptengine",
            "eval",
            "javascript",
            "randomaccessfile",
            "filewriter",
            "bufferedwriter",
            "writeobject",
            "readobject",
            "native",
            "jni",
            "http://",
            "https://",
            "tcp://",
            "udp://",
            "smtp://",
            "ftp://",
            "smb://",
            "tomcat",
            "jetty",
            "undertow",
            "http://",
            "https://",
            "tcp://",
            "udp://",
            "smtp://",
            "ftp://",
            "smb://",
            "bypass",
            "ignoresecurity",
            "disablesecurity",
            "tomcat",
            "jetty",
            "undertow"
        ]
    def _extract_ip_addresses(self, text):
        ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
        ip_addresses = re.findall(ip_pattern, text)

        unique_ip_counts = {}
        for ip in ip_addresses:
            try:
                ip_obj = ipaddress.ip_address(ip)
                if not ip_obj.is_reserved and not ip_obj.is_private:
                    unique_ip_counts[ip] = unique_ip_counts.get(ip, 0) + 1
            except ValueError:
                pass  # Invalid IP address, skip

        total = 0
        for ip in unique_ip_counts:
            total += unique_ip_counts[ip]
        self.results['ip'] = {
            'count': total,
            'addresses':unique_ip_counts
        }
    
    def _extract_jar_header_info(self, file_path):
        jar_info = {}
        try:
            with zipfile.ZipFile(file_path, 'r') as zip_ref:
                jar_info['FileCount'] = len(zip_ref.namelist())
                if 'META-INF/MANIFEST.MF' in zip_ref.namelist():
                    manifest = zip_ref.read('META-INF/MANIFEST.MF').decode('utf-8')
                    manifest_lines = manifest.splitlines()
                    for line in manifest_lines:
                        if ': ' in line:
                            key, value = line.split(': ', 1)
                            jar_info[key] = value
        except Exception as e:
            print(f"Error extracting JAR header info: {e}")
        return jar_info


    # Function to calculate Shannon entropy of a string
    def _calculate_entropy(self, string):
        if not string:
            return 0.0
        entropy = 0
        for character in set(string):
            p_i = float(string.count(character)) / len(string)
            entropy -= p_i * math.log2(p_i)
        return entropy

    # Define a function to calculate relative entropy (Kullback-Leibler divergence)
    def _calculate_relative_entropy(self, string_value):
        # English language character frequency distribution
        english_frequency = {
            'a': 0.08167, 'b': 0.01492, 'c': 0.02782, 'd': 0.04253, 'e': 0.12702,
            'f': 0.02228, 'g': 0.02015, 'h': 0.06094, 'i': 0.06966, 'j': 0.00153,
            'k': 0.00772, 'l': 0.04025, 'm': 0.02406, 'n': 0.06749, 'o': 0.07507,
            'p': 0.01929, 'q': 0.00095, 'r': 0.05987, 's': 0.06327, 't': 0.09056,
            'u': 0.02758, 'v': 0.00978, 'w': 0.0236, 'x': 0.0015, 'y': 0.01974,
            'z': 0.00074, ' ': 0.19181  # Space character is also considered
        }
        
        # Calculate character frequency distribution in the string
        string_frequency = Counter(string_value.lower())
        string_length = len(string_value)
        
        # Calculate Kullback-Leibler divergence
        kl_divergence = 0
        for char, english_prob in english_frequency.items():
            observed_prob = string_frequency[char] / string_length
            if observed_prob > 0:
                kl_divergence += observed_prob * math.log2(observed_prob / english_prob)
        
        return kl_divergence

    def _detect_suspicious_strings(self, text):
        tokens = nltk.word_tokenize(text)
        for i in range(len(tokens)):
            tokens[i] = self._clean_string(tokens[i])
        ## reassemble
        cleaned_text = " ".join(tokens)

        # Create an instance of the language detector
        result = detector.FindLanguage(cleaned_text)
        if result.is_reliable is False and result.probability < 0.40:
            return cleaned_text
        else:
            return False
        
    def _clean_string(self, string):
        string = re.sub(r'[^a-zA-Z0-9]', '', string).lower()
        return string

    # Function to analyze constant pool
    def _analyze_constant_pool(self, java_source_code):
        # Extract strings from the decompiled Java source code
        string_literals = re.findall(r'"([^"]*)"', java_source_code)
        
        for string_literal in string_literals:
            sus_string = self._detect_suspicious_strings(string_literal)
            if sus_string is not False:
                entropy = self._calculate_entropy(string_literal)
                relative_entropy = self._calculate_relative_entropy(string_literal)
                
                # Adjust the thresholds as needed based on your requirements
                if entropy > 4.0 or relative_entropy > 2.0:
                    #print(f"High-entropy and high relative entropy string found: {string_literal}")
                    self.results['high_entropy_strings'] += 1

    # Function to analyze bytecode instructions
    def _analyze_bytecode(self, java_source_code):
        # Parse the Java source code
        try:
            tree = javalang.parse.parse(java_source_code)
        except javalang.parser.JavaSyntaxError as e:
            #print(f"Java syntax error: {e}")
            return

        # Implement bytecode analysis logic here
        # You can traverse the abstract syntax tree (tree) to analyze bytecode instructions
        # Example:
        for path, node in tree:
            if isinstance(node, javalang.tree.MethodInvocation):
                class_str = f"{node.qualifier}".strip().lower()
                method = f"{node.member}".strip().lower()
                #method_call = clean_string(method_call).strip()
                if class_str in self.api_dict:
                    if method in self.api_dict[class_str]:
                        self.results['suspicious_API_calls'] += 1
                        #print(f"Suspicious API invocation found: {class_str}.{method}")

    # Function to analyze empty catch clauses
    def _analyze_empty_catch_clauses(self, java_source_code):
        # Parse the Java source code
        
        try:
            tree = javalang.parse.parse(java_source_code)
        except javalang.parser.JavaSyntaxError as e:
            #print(f"Java syntax error: {e}")
            return

        # Implement empty catch clauses analysis logic here
        # You can traverse the abstract syntax tree (tree) to analyze empty catch clauses
        # Example:
        for path, node in tree:
            if isinstance(node, javalang.tree.CatchClause) and not node.block:
                self.results['empty_catch_clauses'] += 1
                #print(f"Empty catch clause found in method: {node.parameter.name}")

    # Function to detect sensitive keywords in Java source code
    def _detect_sensitive_keywords(self, java_source_code):
        for keyword in self.sensitive_keywords:
            if keyword in java_source_code.lower():
                self.results['sensitive_keywords'] += 1
                #print(f"Sensitive keyword found: {keyword}")

    def _clean_code(self, code):
        # # Handle invalid Unicode escapes and characters by encoding with 'unicode-escape' and 'replace' error handler
        # try:
        #     code = code.encode('utf-8', 'replace').decode('unicode-escape')
        # except UnicodeDecodeError as e:
        #     print(f"UnicodeDecodeError: {e}")

        emoji_pattern = re.compile(
            r"["
            u"\U0001F600-\U0001F64F"  # Emoticons
            u"\U0001F300-\U0001F5FF"  # Symbols & Pictographs
            u"\U0001F680-\U0001F6FF"  # Transport & Map Symbols
            u"\U0001F700-\U0001F77F"  # Alchemical Symbols
            u"\U0001F780-\U0001F7FF"  # Geometric Shapes Extended
            u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
            u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
            u"\U0001FA00-\U0001FA6F"  # Chess Symbols
            u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
            u"\U0001F004-\U0001F0CF"  # Additional Emoticons
            u"\U0001F170-\U0001F251"  # Enclosed Ideographic Supplement
            u"\u2600-\u26FF"  # Miscellaneous Symbols
            u"\u2700-\u27BF"  # Dingbats
            "]+",
            flags=re.UNICODE,
        )

        # Use regex to remove emojis
        code_no_emojis = emoji_pattern.sub(r"", code)
        return code_no_emojis

    def _replace_unicode_with_ascii(self, input_text):
        input_text = input_text.replace('\\u','\\.u')

        # Create an empty string to store the result
        result = ""

        for char in input_text:
            safe_list = [9,10]
            if (ord(char) > 127 or ord(char) < 32) and ord(char) not in safe_list:  # Check if the character is non-ASCII
                # Replace the non-ASCII character with its hexadecimal Unicode value in ASCII
                result += "u{:04x}".format(ord(char))
            else:
                result += char

        # with open("clean_java", 'w') as file:
        #     file.write(result)
        return result

    # Function to analyze a Java source file
    def _analyze_java_file(self, file_path):
        #file_path = "java_decompiled/6d9a4ff149a2a97ed05940bd4ef368c26bb8faf69bac2cbca7c01ce201c0d00c/net/minecraft/xs.java"
        with open(file_path, "r") as file:
            java_source_code = file.read()
        
        # with open("dirty_java", 'w') as file:
        #     file.write(java_source_code)
        # print(file_path)

        ## clean java_source_code
        java_source_code = self._replace_unicode_with_ascii(java_source_code)


        self._analyze_constant_pool(java_source_code)
        self._analyze_bytecode(java_source_code)
        self._analyze_empty_catch_clauses(java_source_code)

        # Detect sensitive keywords in the Java source code
        self._detect_sensitive_keywords(java_source_code)

        # find IPs
        self._extract_ip_addresses(java_source_code)

    # Recursively find and analyze all .java files in a directory
    def static_analysis(self, file_path, hash_value):
        self.results = {
            'high_entropy_strings': 0,
            'suspicious_API_calls': 0,
            'empty_catch_clauses': 0,
            'sensitive_keywords': 0,
            'jar_info':self._extract_jar_header_info(file_path)
        }
        directory = f"java_decompiled/{hash_value}"
        for root, _, files in os.walk(directory):
            for file in files:
                if file.endswith(".java"):
                    file_path = os.path.join(root, file)
                    #print(f"Analyzing: {file_path}")
                    self._analyze_java_file(file_path)
        return self.results
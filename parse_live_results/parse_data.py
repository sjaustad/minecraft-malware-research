import os,json, requests, sys
import pandas as pd
from web_cpu_memory_usage import extract_usage_data

csv_filename = "sample_list.csv"
ip_lookups = []
api_requests = 0

## Function to avoid duplicate API calls for already located IP addresses
def find_ip_in_list(ip_lookups, target_ip):
    matching_entries = [entry for entry in ip_lookups if entry.get('ip') == target_ip]
    return matching_entries[0] if matching_entries else None


def import_json_files(folder_path):
    json_data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.json'):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                try:
                    data = json.load(file)
                    data['sha256'] = os.path.splitext(filename)[0]
                    json_data.append(data)
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON file {filename}: {e}")
    return json_data

def find_ip_region(ip):
    api_key = "eb4440dff22ff3ebfe266edbbaafd189"

    base_url = f"http://api.ipstack.com/{ip}?access_key={api_key}"

    ## see if we've already seen this IP before to avoid duplicate API calls
    check_ip = find_ip_in_list(ip_lookups, ip)

    if check_ip is None:
        # run lookup request
        response = requests.get(base_url)
        data = response.json()
        ip_lookups.append(data)
        global api_requests 
        api_requests += 1
        if api_requests % 25 == 0:
            print(f"Requests: {api_requests}")
    else:
        data = check_ip

    if 'region_name' in data:
        return data
    else:
        return None
def build_process_tree(processes, root_pid):
    

    root_process = None
    # Find the root processes based on the provided PIDs
    for process in processes:
        if process['pid'] == root_pid:
            root_process = process
            break
    
    if not root_process:
        
        return None
    
    
    visited = set()  # Set to keep track of visited processes
    
    # Recursively find children processes
    def find_children(parent_process):
        children = []
        
        for process in processes:
            if process['ppid'] == parent_process['pid'] and process['pid'] not in visited:
                visited.add(process['pid'])
                children.append(process)
        
        for child in children:
            child['children'] = find_children(child)

        return children
        

    root_process['children'] = find_children(root_process)
    return root_process

def find_process_triggers(processes, pid):
    binary_calls = []

    def recursive_find(process):
        binary_calls.append(process['fileName'])

        if 'modules' in process:
            for module in process['modules']:
                if 'image' in module:
                    module_image = os.path.basename(module['image'])
                    binary_calls.append(module_image)

        if 'children' in process:
            for child_process in process['children']:
                recursive_find(child_process)

    for process in processes:
        if process['pid'] == pid:
            recursive_find(process)

    return binary_calls

def process_usage_metrics(metric_list):
    high_range = 0
    low_range = 28
    refactored_metrics = []
    for metric in metric_list:
        value = 1 - (metric/low_range)
        refactored_metrics.append(value)
    
    return refactored_metrics

def parse_analysis(run):
    data = {}

    # Find the corresponding row based on the given hash value
    matching_row = df[df['Hash'] == run['sha256']]

    if matching_row.empty:
        return None

    root_pid = int(matching_row['PID'].iloc[0])
    root_file_name = matching_row['Filename'].iloc[0]
    
    ## Build process tree
    data['application'] = {
        'name':root_file_name,
        'pid':root_pid,
        'hash':run['sha256'],#run['analysis']['content']['mainObject']['hashes']['sha256'],
        'duration':run['analysis']['duration'],
        'url':run['analysis']['permanentUrl']
    }
    data['application']['root_process'] = build_process_tree(run['processes'], root_pid)
    if data['application']['root_process'] is None:
        print(f"Warning, could not find PID {root_pid} for {data['application']['hash']}")
        return None

    ## network
    data['network'] = {}
    data['network']['counters'] = {
        'dns':run['counters']['network']['dns'],
        'connections':run['counters']['network']['connections'],
        'http':run['counters']['network']['http']
    }
    # aggregate all network traffic
    data['network'] = []
    for entry in run['network']['dnsRequests']:
        for ip in entry['ips']:
            data['network'].append({'ip':ip,'reputation':entry['reputation'],'type':'dns'})
    for entry in run['network']['httpRequests']:
        data['network'].append({'ip':entry['ip'],'reputation':entry['reputation'],'port':entry['port'],'type':'http'})
    for entry in run['network']['connections']:
        data['network'].append({'ip':entry['ip'],'reputation':entry['reputation'], 'port':entry['port'],'type':'connection'})
    
    # get geo region of IPs
    for ip in data['network']:
        ip['region'] = find_ip_region(ip['ip'])

    ## System Calls
    data['sys_calls'] = {
        'files':{},
        'registry':{}
    }
    data['sys_calls']['files']['modified_count'] = len(run['modified']['files'])


    data['sys_calls']['registry']['read'] = run['counters']['registry']['read']
    data['sys_calls']['registry']['write'] = run['counters']['registry']['write']
    data['sys_calls']['registry']['delete'] = run['counters']['registry']['delete']

    data['sys_calls']['binary_triggers'] = find_process_triggers(run['processes'], root_pid)

    ## memory
    data['memory'] = {
        'dll_list':[]
    }
    for binary in data['sys_calls']['binary_triggers']:
        if ".dll" in binary:
            data['memory']['dll_list'].append(binary)

    ## check for changes to startup files
    data['persistence'] = {
        'suspicious_files':[],
        'suspicious_binaries':[]
    }
    for file_event in run['modified']['files']:
        sus_strings = ['Start Menu','StartUp']
        for string in sus_strings:
            if string.lower() in file_event['filename'].lower():
                data['persistence']['suspicious_files'].append(file_event)
                break
    
    for binary_event in data['sys_calls']['binary_triggers']:
        sus_binaries = ['Reg.exe', 'Nslookup.exe', 'Regasm.exe', 'Runas.exe', 'Schtasks.exe', 'Sc.exe']
        for binary in sus_binaries:
            if binary.lower() == binary_event.lower():
                data['persistence']['suspicious_binaries'].append(binary)

    ## get web information
    data['anomaly_detection']={
        'cpu':{},
        'memory':{}
    }
    cpu_usage, memory_usage = extract_usage_data(data['application']['url'])
    ## convert metrics to percentages
    data['anomaly_detection']['cpu']['usage'] = process_usage_metrics(cpu_usage)
    data['anomaly_detection']['memory']['usage'] = process_usage_metrics(memory_usage)

    return data

# Load the CSV file using pandas
df = pd.read_csv(csv_filename)


    


# Provide the folder path to the "json" folder
source_folder_path = os.path.join(os.path.dirname(__file__), 'json_data')
  # Update this path according to your folder structure
results_folder_path = os.path.join(os.path.dirname(__file__), 'parsed_analyses')

# Call the function and get the list of dictionaries
json_list = import_json_files(source_folder_path)

if not os.path.exists(results_folder_path):
    os.makedirs(results_folder_path)

# Print the imported JSON data
for json_dict in json_list:
    ## results file path
    file_path = os.path.join(results_folder_path,f"{json_dict['sha256']}.json")

    if os.path.exists(file_path):
        print(f"Already processed {json_dict['sha256']}")
        continue
    
    #file_path = os.path.join(results_folder_path, f"{data['application']['hash']}.json")
    print(f"Processing {json_dict['sha256']}")
    
    ## analyze file
    data = parse_analysis(json_dict)
    

    with open(file_path, 'w') as file:
        json.dump(data, file)

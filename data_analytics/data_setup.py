import pandas as pd
import numpy as np
import ast,copy
import seaborn as sns
import matplotlib.pyplot as plt

jar_columns = [
    'static.high_entropy_strings',
    'static.suspicious_API_calls',
    'static.empty_catch_clauses',
    'static.sensitive_keywords',
    'static.jar_info.FileCount',
    #'static.ip.count', # removed for not having useful data
]
exe_columms = [
    'static.ip_addresses.count',
    'static.urls.count',
    'static.dll_strings.count',
    'static.header.imported_functions.count',
    'live.persistence.suspicious_files_count',
    #'static.header.suspicious_packer', # Removed for not having useful data
]

class DataSetup:
    def __init__(self, df, sample_type):
        self.columns = [
            'live.network.type.dns',
            'live.network.type.http',
            'live.network.type.connection',
            'live.sys_calls.files.modified_count',
            'live.sys_calls.registry.read',
            'live.sys_calls.registry.write',
            'live.sys_calls.registry.delete',
            'live.memory.dll_count',
            #'live.persistence.suspicious_files_count', # removed here because it wasn't useful for jars, added to exes
            'live.persistence.suspicious_binaries_count',
            'live.anomaly_detection.cpu.mean',
            'live.anomaly_detection.memory.mean',
            'VirusTotal.malicious'
        ]

        ## add region names
        # Iterate through the columns of the DataFrame
        for column in df.columns:
            if column.startswith('live.network.regions'):
                self.columns.append(column)

        if sample_type == 'jar':
            self.columns.extend(jar_columns)
            self.type = 'jar'
        elif sample_type == 'exe' or sample_type == 'msi':
            self.columns.extend(exe_columms)
            self.type = 'exe'

        new_df = df[self.columns]


        self.train_set, self.test_set = self._separate_training_values(new_df)
    
    def _separate_training_values(self, data_df):
        # Define the percentage split (70% training, 30% testing)
        train_percent = 0.7
        test_percent = 1 - train_percent

        data_df = data_df.fillna(0)

        # Calculate the number of rows for each split
        total_rows = len(data_df)
        train_size = int(train_percent * total_rows)
        test_size = total_rows - train_size

        # Set a consistent seed value for reproducibility
        seed_value = 69420

        # Randomly split the DataFrame into two with the specified seed
        train_df = data_df.sample(n=train_size, random_state=seed_value)
        test_df = data_df.drop(train_df.index)


        # Reset the index for both DataFrames
        train_df.reset_index(drop=True, inplace=True)
        test_df.reset_index(drop=True, inplace=True)
        return train_df, test_df

    def calculate_summary_statistics(self, summary_df, output_csv_path):

        # Calculate the mean and standard deviation for each column
        summary_data = pd.DataFrame({
            'Column_Name': summary_df.columns,
            'Mean': summary_df.mean(),
            'Std_Deviation': summary_df.std()
        })
        
        # Save the summary data to a CSV file in the 'data_analytics' directory
        summary_data.to_csv(output_csv_path, index=False)
        print(f"Summary data saved to {output_csv_path}")

    def generate_correlation_matrix(self, corr_df):
        corr_df_copy = corr_df.copy()

        # Custom function to aggregate values in columns starting with 'live.network.regions'
        def aggregate_regions(row):
            region_columns = [col for col in row.index if col.startswith('live.network.regions')]
            return row[region_columns].sum()

        # Apply the aggregation function to create a new 'live.network.regions' column
        corr_df_copy['live.network.all_regions_count'] = corr_df_copy.apply(aggregate_regions, axis=1)



        # Drop the original region columns
        corr_df_copy.drop([col for col in corr_df_copy.columns if col.startswith('live.network.regions')], axis=1, inplace=True)

        # Calculate the correlation matrix
        matrix = corr_df_copy.corr()


        plt.figure(figsize=(10,8))
        sns.heatmap(matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"fontsize": 9}, square=True)

        # save the heatmap
        plt.title('Correlation Heatmap')
        plt.savefig(f'data_analytics/{self.type}_heatmap.png', bbox_inches='tight')

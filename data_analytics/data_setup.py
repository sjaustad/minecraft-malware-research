import pandas as pd
import numpy as np
import ast,copy
import seaborn as sns
import matplotlib.pyplot as plt

jar_columns = [
    'static.high_entropy_strings',
    'static.suspicious_API_calls',
    'static.empty_catch_clauses',
    'static.sensitive_keywords',
    'static.jar_info.FileCount',
    #'static.ip.count', # removed for not having useful data
]
exe_columms = [
    'static.ip_addresses.count',
    'static.urls.count',
    'static.dll_strings.count',
    'static.header.imported_functions.count',
    'live.persistence.suspicious_files_count',
    #'static.header.suspicious_packer', # Removed for not having useful data
]

class DataSetup:
    def __init__(self, df, sample_type):
        self.columns = [
            'live.network.type.dns',
            'live.network.type.http',
            'live.network.type.connection',
            'live.sys_calls.files.modified_count',
            'live.sys_calls.registry.read',
            'live.sys_calls.registry.write',
            'live.sys_calls.registry.delete',
            'live.memory.dll_count',
            #'live.persistence.suspicious_files_count', # removed here because it wasn't useful for jars, added to exes
            'live.persistence.suspicious_binaries_count',
            'live.anomaly_detection.cpu.mean',
            'live.anomaly_detection.memory.mean',
            'VirusTotal.malicious',
            'VirusTotal.detections'
        ]

        ## add region names
        # Iterate through the columns of the DataFrame
        for column in df.columns:
            if column.startswith('live.network.regions'):
                self.columns.append(column)

        if sample_type == 'jar':
            self.columns.extend(jar_columns)
            self.type = 'jar'

        elif sample_type == 'exe' or sample_type == 'msi':
            self.columns.extend(exe_columms)
            self.type = 'exe'

        for row in df.index:
            if df['VirusTotal.detections'][row] <= 0:
                df['VirusTotal.malicious'][row] = 0
            else:
                df['VirusTotal.malicious'][row] = 1

        # df = df[df['live.application.child_depth'] > 1]
        # print(df['live.application.child_depth'].tolist())
        # print(df.shape)
        new_df = df[self.columns]

        ## Make data ratioed
        # if self.type == 'jar':
        #     for row in new_df.index:
        #         for col in jar_columns:
        #             if col != 'static.jar_info.FileCount':
        #                 new_df[col][row] = new_df[col][row] / new_df['static.jar_info.FileCount'][row]
            #print(new_df)

        # for row in new_df.index:
        #     if new_df['VirusTotal.detections'][row] <= 0:
        #         new_df['VirusTotal.malicious'][row] = 0
        #     else:
        #         new_df['VirusTotal.malicious'][row] = 1
        self.train_set, self.test_set = self._separate_training_values(new_df)



    def _separate_training_values(self, data_df):
        # Define the percentage split (70% training, 30% testing)
        train_percent = 0.7
        test_percent = 1 - train_percent

        data_df = data_df.fillna(0)

        # Calculate the number of rows for each split
        total_rows = len(data_df)
        train_size = int(train_percent * total_rows)
        test_size = total_rows - train_size

        # Set a consistent seed value for reproducibility
        if self.type == 'jar':
            seed_value = 1111            
        elif self.type == 'exe':
            # 5:15 - train
            # 3:6 - test
            seed_value = 1000000
            # # Tolerance range for each condition
            # tolerance = 0
            # tolerance2 = 0
            # # Define the search range
            # min_seed = 1000000
            # max_seed = 10000000

            # # Desired split conditions without tolerance
            # desired_train_split_1 = 5
            # desired_train_split_0 = 15
            # desired_test_split_1 = 3
            # desired_test_split_0 = 6

            # while min_seed <= max_seed:
            #     # Use binary search to narrow down the search space
            #     seed_value = (min_seed + max_seed) // 2
                
            #     # Split the data into train and test based on the generated seed
            #     train_df = data_df.sample(n=train_size, random_state=seed_value)
            #     test_df = data_df.drop(train_df.index).reset_index(drop=True) 
                
            #     # Calculate the counts of 'VirusTotal.malicious' values in train and test sets
            #     train_count_1 = train_df['VirusTotal.malicious'].sum()
            #     train_count_0 = len(train_df) - train_count_1
            #     test_count_1 = test_df['VirusTotal.malicious'].sum()
            #     test_count_0 = len(test_df) - test_count_1
                
            #     # Check if the split conditions are met exactly
            #     if (
            #         train_count_1 == desired_train_split_1 and
            #         train_count_0 == desired_train_split_0 and
            #         test_count_1 == desired_test_split_1 and
            #         test_count_0 == desired_test_split_0
            #     ):
            #         break
            #     elif (
            #         train_count_1 > desired_train_split_1 or
            #         train_count_0 > desired_train_split_0 or
            #         test_count_1 > desired_test_split_1 or
            #         test_count_0 > desired_test_split_0
            #     ):
            #         # Adjust the search range if the current seed is too high
            #         max_seed = seed_value - 1
            #     else:
            #         # Adjust the search range if the current seed is too low
            #         min_seed = seed_value + 1

            # print(f"Found seed value: {seed_value}")
            # print(f"Train Split - Malicious: {train_count_1}, Benign: {train_count_0}")
            # print(f"Test Split - Malicious: {test_count_1}, Benign: {test_count_0}")

        # Randomly split the DataFrame into two with the specified seed

        train_df = data_df.sample(n=train_size, random_state=seed_value)
        test_df = data_df.drop(train_df.index)


        # Reset the index for both DataFrames
        train_df.reset_index(drop=True, inplace=True)
        test_df.reset_index(drop=True, inplace=True)
        return train_df, test_df
        
    def calculate_summary_statistics(self, summary_df, output_csv_path):
        # Split the DataFrame into two based on 'VirusTotal.malicious' column
        malicious_df = summary_df[summary_df['VirusTotal.malicious'] == 1]
        non_malicious_df = summary_df[summary_df['VirusTotal.malicious'] == 0]

        # Create a list of columns to consider
        columns_to_consider = []
        for column in summary_df.columns:
            if column.startswith('live.network.regions'):
                columns_to_consider.append(column)

        # Calculate the mean for each group
        malicious_mean = malicious_df.mean().round(2)
        non_malicious_mean = non_malicious_df.mean().round(2)

        # Truncate the top 5 network regions for each DataFrame
        for col in columns_to_consider:
            malicious_df[col] = malicious_df[col].nlargest(5)
            non_malicious_df[col] = non_malicious_df[col].nlargest(5)

        # Create a DataFrame to store the results for network regions
        result_df = pd.DataFrame({
            'Feature': columns_to_consider,
            'Non-malicious': non_malicious_mean[columns_to_consider],
            'Malicious': malicious_mean[columns_to_consider]
        })

        # Add standard deviations in parentheses for network regions
        result_df['Non-malicious'] = result_df['Non-malicious'].astype(str) + ' (' + non_malicious_df[columns_to_consider].std().round(2).astype(str) + ')'
        result_df['Malicious'] = result_df['Malicious'].astype(str) + ' (' + malicious_df[columns_to_consider].std().round(2).astype(str) + ')'

        # Concatenate with the means for other columns
        other_columns = [col for col in summary_df.columns if col not in columns_to_consider]
        result_df_other = pd.DataFrame({
            'Feature': other_columns,
            'Non-malicious': non_malicious_mean[other_columns],
            'Malicious': malicious_mean[other_columns]
        })

        # Add standard deviations in parentheses for other columns
        result_df_other['Non-malicious'] = result_df_other['Non-malicious'].astype(str) + ' (' + non_malicious_df[other_columns].std().round(2).astype(str) + ')'
        result_df_other['Malicious'] = result_df_other['Malicious'].astype(str) + ' (' + malicious_df[other_columns].std().round(2).astype(str) + ')'

        # Concatenate the DataFrames for network regions and other columns
        result_df = pd.concat([result_df, result_df_other], ignore_index=True)

        # Sort the DataFrame by the 'Malicious' column in descending order
        result_df = result_df.sort_values(by='Malicious', ascending=False)

        # Save the summary data to a CSV file
        result_df.to_csv(output_csv_path, index=False)
        print(f"Summary data saved to {output_csv_path}")


    def generate_correlation_matrix(self, corr_df):
        corr_df_copy = corr_df.copy()

        # Custom function to aggregate values in columns starting with 'live.network.regions'
        def aggregate_regions(row):
            region_columns = [col for col in row.index if col.startswith('live.network.regions')]
            return row[region_columns].sum()

        # Apply the aggregation function to create a new 'live.network.regions' column
        corr_df_copy['live.network.all_regions_count'] = corr_df_copy.apply(aggregate_regions, axis=1)



        # Drop the original region columns
        corr_df_copy.drop([col for col in corr_df_copy.columns if col.startswith('live.network.regions')], axis=1, inplace=True)

        # Calculate the correlation matrix
        matrix = corr_df_copy.corr()


        plt.figure(figsize=(10,8))
        sns.heatmap(matrix, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"fontsize": 9}, square=True)

        # save the heatmap
        plt.title('Correlation Heatmap')
        plt.savefig(f'data_analytics/{self.type}_heatmap.png', bbox_inches='tight')

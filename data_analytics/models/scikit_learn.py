## To do:
# visual representation of decision tree
# look into ks nearest neighbor instead of k-means

# scikit feature selection code: https://scikit-learn.org/stable/modules/feature_selection.html
# 1.13.2
# Check how to split the data in train and corss and use this to validate
# 10 fold cross validation

# Run three different tests
# 1. with all features
# 2. test removing 0.90 and -0.90
# 3. test removing 0.80 and -0.80

# Make single graph with ROC graph to compare all the algorithms

# Make results look like these:
# Google Scholar search: malware classification machine learning source:computers source:& source:security
# https://www.sciencedirect.com/science/article/abs/pii/S0167404820301462
# https://www.sciencedirect.com/science/article/pii/S0167404817302535
# https://www.sciencedirect.com/science/article/abs/pii/S0167404821003394
# https://www.sciencedirect.com/science/article/abs/pii/S0167404820301681
# https://www.sciencedirect.com/science/article/abs/pii/S0167404818303481

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier


import matplotlib.pyplot as plt
import seaborn as sns


class Scikit:
    def __init__(self):
        pass

    def classification_tree(self, training_df, test_df):
        # 1. Split the training and test datasets into features (X) and target (y)
        X_train = training_df.drop('VirusTotal.malicious', axis=1)
        y_train = training_df['VirusTotal.malicious']
        X_test = test_df.drop('VirusTotal.malicious', axis=1)
        y_test = test_df['VirusTotal.malicious']

        # 2. Create and train a Decision Tree Classifier on the training data
        clf = DecisionTreeClassifier(random_state=42)
        clf.fit(X_train, y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Print or use these metrics as needed
        print(f'Accuracy: {accuracy:.2f}')
        print(f'Precision: {precision:.2f}')
        print(f'Recall: {recall:.2f}')
        print(f'F1 Score: {f1:.2f}')

    def neural_network(self, training_df, test_df):
        # 1. Split the training and test datasets into features (X) and target (y)
        X_train = training_df.drop('VirusTotal.malicious', axis=1)
        y_train = training_df['VirusTotal.malicious']
        X_test = test_df.drop('VirusTotal.malicious', axis=1)
        y_test = test_df['VirusTotal.malicious']

        # 2. Create and train a Multi-Layer Perceptron (Neural Network) Classifier
        clf = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42)
        clf.fit(X_train, y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Print or use these metrics as needed
        print(f'Accuracy: {accuracy:.2f}')
        print(f'Precision: {precision:.2f}')
        print(f'Recall: {recall:.2f}')
        print(f'F1 Score: {f1:.2f}')

    def naive_bayes(self, training_df, test_df):
        # 1. Split the training and test datasets into features (X) and target (y)
        X_train = training_df.drop('VirusTotal.malicious', axis=1)
        y_train = training_df['VirusTotal.malicious']
        X_test = test_df.drop('VirusTotal.malicious', axis=1)
        y_test = test_df['VirusTotal.malicious']

        # 2. Create and train a Gaussian Naive Bayes Classifier
        clf = GaussianNB()
        clf.fit(X_train, y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Print or use these metrics as needed
        print(f'Accuracy: {accuracy:.2f}')
        print(f'Precision: {precision:.2f}')
        print(f'Recall: {recall:.2f}')
        print(f'F1 Score: {f1:.2f}')

    def k_means(self, df, n_clusters=2):
        # Create features (X) by excluding the target column ('VirusTotal.malicious')
        X = df.drop('VirusTotal.malicious', axis=1)

        # Create a K-Means model with the specified number of clusters
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        
        # Fit the K-Means model to the data
        kmeans.fit(X)

        # Add cluster labels to the DataFrame (optional)
        df['Cluster'] = kmeans.labels_

        # Visualize the results (optional)
        # Note: Visualization may not be feasible for high-dimensional data
        # You may need to reduce dimensionality before visualization
        # Here, we'll just show the first two principal components
        if X.shape[1] > 2:
            from sklearn.decomposition import PCA
            pca = PCA(n_components=2)
            X_pca = pca.fit_transform(X)
            plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans.labels_, cmap='viridis')
            plt.xlabel('Principal Component 1')
            plt.ylabel('Principal Component 2')
        else:
            plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=kmeans.labels_, cmap='viridis')
            plt.xlabel(X.columns[0])
            plt.ylabel(X.columns[1])
        
        plt.show()
        
    def support_vector_machine(self, training_df, test_df):
        # 1. Split the training and test datasets into features (X) and target (y)
        X_train = training_df.drop('VirusTotal.malicious', axis=1)
        y_train = training_df['VirusTotal.malicious']
        X_test = test_df.drop('VirusTotal.malicious', axis=1)
        y_test = test_df['VirusTotal.malicious']

        # 2. Create and train an SVM Classifier
        clf = SVC(kernel='linear', random_state=42)
        clf.fit(X_train, y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        # Print or use these metrics as needed
        print(f'Accuracy: {accuracy:.2f}')
        print(f'Precision: {precision:.2f}')
        print(f'Recall: {recall:.2f}')
        print(f'F1 Score: {f1:.2f}')

    def random_forest(self, training_df, test_df):
        # 1. Split the training and test datasets into features (X) and target (y)
        X_train = training_df.drop('VirusTotal.malicious', axis=1)
        y_train = training_df['VirusTotal.malicious']
        X_test = test_df.drop('VirusTotal.malicious', axis=1)
        y_test = test_df['VirusTotal.malicious']

        # 2. Create and train a Random Forest Classifier
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        clf.fit(X_train, y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test,y_pred)
        # Print or use these metrics as needed
        print(f'Accuracy: {accuracy:.2f}')
        print(f'Precision: {precision:.2f}')
        print(f'Recall: {recall:.2f}')
        print(f'F1 Score: {f1:.2f}')

    def generate_confusion_matric(self, y_test, y_pred):
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.show()
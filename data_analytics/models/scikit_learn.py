## To do:
# visual representation of decision tree
# look into ks nearest neighbor instead of k-means

# scikit feature selection code: https://scikit-learn.org/stable/modules/feature_selection.html
# 1.13.2
# Check how to split the data in train and corss and use this to validate
# 10 fold cross validation

# Run three different tests
# 1. with all features
# 2. test removing 0.90 and -0.90
# 3. test removing 0.80 and -0.80

# Make single graph with ROC graph to compare all the algorithms

# Make results look like these:
# Google Scholar search: malware classification machine learning source:computers source:& source:security
# https://www.sciencedirect.com/science/article/abs/pii/S0167404820301462
# https://www.sciencedirect.com/science/article/pii/S0167404817302535
# https://www.sciencedirect.com/science/article/abs/pii/S0167404821003394
# https://www.sciencedirect.com/science/article/abs/pii/S0167404820301681
# https://www.sciencedirect.com/science/article/abs/pii/S0167404818303481

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.datasets import load_iris



import matplotlib.pyplot as plt
import seaborn as sns

from copy import deepcopy

class Scikit:
    def __init__(self, train_df, test_df, correlation_filter = 0, k_best_features=0):
        columns = ['classifier','accuracy', 'precision', 'recall', 'f1_score','cv_avg','cv_std_dev']
        self.results_df = pd.DataFrame(columns=columns)
        #self.train_df = train_df.copy()
        #self.test_df = test_df.copy()
        if correlation_filter > 0:
            self.train_df = self._remove_highly_correlated_features(train_df, correlation_filter)
            common_columns = self.train_df.columns.intersection(test_df.columns)
            self.test_df = test_df[common_columns]
        else:
            self.train_df = train_df
            self.test_df = test_df

        self.train_df = self.train_df.drop('VirusTotal.detections', axis=1)
        self.test_df = self.test_df.drop('VirusTotal.detections', axis=1)

        # 1. Split the training and test datasets into features (X) and target (y)
        self.X_train = self.train_df.drop('VirusTotal.malicious', axis=1)
        self.y_train = self.train_df['VirusTotal.malicious']
        self.X_test = self.test_df.drop('VirusTotal.malicious', axis=1)
        self.y_test = self.test_df['VirusTotal.malicious']

        try:
            if k_best_features > 0:
                # Add a parameter for the number of top features to select
                self.k_best_features = k_best_features
                # Perform feature selection using SelectKBest with f_classif
                self.X_train = self._select_k_best_features(self.X_train, self.y_train, self.k_best_features)
                self.X_test = self.X_test[self.X_train.columns]  # Apply the same feature selection to the test data
        except:
            new_row = {'classifier': 'FAILED', 'accuracy':'FAILED','precision':'FAILED','recall':'FAILED','f1_score':'FAILED','cv_avg':'FAILED','cv_std_dev':'FAILED'}
            self.results_df.loc[len(self.results_df.index)] = new_row
            return

        
        self.classification_tree()
        self.neural_network()
        self.naive_bayes()
        self.knn_classification()
        self.support_vector_machine()
        self.random_forest()


    
    def _select_k_best_features(self, X, y, k):
        # Create a SelectKBest object using f_classif scoring
        selector = SelectKBest(score_func=f_classif, k=k)

        # Fit and transform the selector on the training data
        X_selected = selector.fit_transform(X, y)

        # Create a DataFrame with the selected features
        selected_feature_indices = selector.get_support(indices=True)
        selected_features = X.columns[selected_feature_indices]
        X_selected_df = pd.DataFrame(X_selected, columns=selected_features)

        return X_selected_df

    def _remove_highly_correlated_features(self, df, threshold):
        # Step 1: Calculate the correlation matrix
        correlation_matrix = df.corr()

        # Step 2: Find highly correlated features
        highly_correlated = set()
        for i in range(len(correlation_matrix.columns)):
            for j in range(i):
                correlation_value = abs(correlation_matrix.iloc[i, j])
                if correlation_value >= threshold:  # Adjust the threshold as needed
                    colname_i = correlation_matrix.columns[i]
                    colname_j = correlation_matrix.columns[j]
                    if colname_i.lower() != 'virustotal.malicious' and colname_j.lower() != 'virustotal.malicious':
                        highly_correlated.add(colname_i)
                        highly_correlated.add(colname_j)
                    else:
                        pass
        print(highly_correlated)
        # Step 3: Drop the highly correlated features
        df_filtered = df.drop(columns=highly_correlated)
        return df_filtered

    def classification_tree(self):

        # 2. Create and train a Decision Tree Classifier on the training data
        clf = DecisionTreeClassifier(random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)


        classifier = 'classification tree'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row

        print(new_row)
        pass
        # Print or use these metrics as needed
        # print(f'Accuracy: {accuracy:.2f}')
        # print(f'Precision: {precision:.2f}')
        # print(f'Recall: {recall:.2f}')
        # print(f'F1 Score: {f1:.2f}')

    def neural_network(self):

        # 2. Create and train a Multi-Layer Perceptron (Neural Network) Classifier
        clf = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'neural network'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row

    def naive_bayes(self):

        # 2. Create and train a Gaussian Naive Bayes Classifier
        clf = GaussianNB()
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'naives bayes'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row

    def knn_classification(self, k=5):

        # 2. Create and train a K-Nearest Neighbors Classifier on the training data
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = knn.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'KNN'
        ## 10 fold cross validation
        cv_scores = cross_val_score(knn,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row

    def support_vector_machine(self):

        # 2. Create and train an SVM Classifier
        clf = SVC(kernel='linear', random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'SVM'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row 

    def random_forest(self):

        # 2. Create and train a Random Forest Classifier
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test,y_pred)

        classifier = 'random forest'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row

    def generate_confusion_matric(self, y_test, y_pred):
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.show()
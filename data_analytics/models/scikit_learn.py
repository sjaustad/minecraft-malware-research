
import os
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.datasets import load_iris
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.calibration import CalibratedClassifierCV



import matplotlib.pyplot as plt
import seaborn as sns

from copy import deepcopy

class Scikit:
    def __init__(self, file_type, test_name,  train_df, test_df, correlation_filter = 0, k_best_features=0):
        columns = ['classifier','accuracy', 'precision', 'recall', 'f1_score','cv_avg','cv_std_dev']
        self.results_df = pd.DataFrame(columns=columns)
        self.roc_curves = []  # To store the ROC curves
        self.auc_scores = []  # To store the AUC scores
        self.classifier_names = []  # To store classifier names

        #self.train_df = train_df.copy()
        #self.test_df = test_df.copy()

        
        if correlation_filter > 0:
            self.train_df = self._remove_highly_correlated_features(train_df, correlation_filter)
            common_columns = self.train_df.columns.intersection(test_df.columns)
            self.test_df = test_df[common_columns]
        else:
            self.train_df = train_df
            self.test_df = test_df
        # For debugging
        # with open('debug.txt','a+') as file:
        #     file.write(f'shape_before: {train_df.shape} : {test_df.shape}\n')
        #
        # with open('debug.txt','a+') as file:
        #     file.write(f'shape after corr filter: {self.train_df.shape} : {self.test_df.shape}\n')
        # ... K filter function ...
        # with open('debug.txt','a+') as file:
        #     file.write(f'after k filter: {self.X_train.shape} : {self.X_test.shape}\n')
        
        self.train_df = self.train_df.drop('VirusTotal.detections', axis=1)
        self.test_df = self.test_df.drop('VirusTotal.detections', axis=1)

        # 1. Split the training and test datasets into features (X) and target (y)
        self.X_train = self.train_df.drop('VirusTotal.malicious', axis=1)
        self.y_train = self.train_df['VirusTotal.malicious']
        self.X_test = self.test_df.drop('VirusTotal.malicious', axis=1)
        self.y_test = self.test_df['VirusTotal.malicious']

        try:
            if k_best_features > 0:
                # Add a parameter for the number of top features to select
                self.k_best_features = k_best_features
                # Perform feature selection using SelectKBest with f_classif
                self.X_train = self._select_k_best_features(self.X_train, self.y_train, self.k_best_features)
                self.X_test = self.X_test[self.X_train.columns]  # Apply the same feature selection to the test data
        except:
            new_row = {'classifier': 'NOT RUN', 'accuracy':'NOT RUN','precision':'NOT RUN','recall':'NOT RUN','f1_score':'NOT RUN','cv_avg':'NOT RUN','cv_std_dev':'NOT RUN'}
            self.results_df.loc[len(self.results_df.index)] = new_row
            return
        
        print('class tree')
        self.classification_tree()
        print('nn')
        self.neural_network()
        print('nb')
        self.naive_bayes()
        print('kn')
        self.knn_classification()
        print('svm')
        self.support_vector_machine()
        print('rf')
        self.random_forest()

        self._plot_roc_graphs(file_type, test_name)

    def _plot_roc_graphs(self, file_type, test_name):
        file_name = f'data_analytics/roc_curve_{file_type}_{test_name}.png'


        plt.figure(figsize=(8, 6))

        # Plot individual ROC curves for each classifier
        for i in range(len(self.roc_curves)):
            fpr, tpr = self.roc_curves[i]
            plt.plot(fpr, tpr, label=f'{self.classifier_names[i]} (AUC = {self.auc_scores[i]:.2f})')

        # Plot the diagonal line (random classifier)
        plt.plot([0, 1], [0, 1], linestyle='--', color='gray')

        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Aggregate ROC Curve for Classifiers')
        plt.legend(loc='lower right')
        plt.savefig(file_name)
        plt.close()


    def _select_k_best_features(self, X, y, k):
        # Create a SelectKBest object using f_classif scoring
        selector = SelectKBest(score_func=f_classif, k=k)

        # Fit and transform the selector on the training data
        X_selected = selector.fit_transform(X, y)

        # Create a DataFrame with the selected features
        selected_feature_indices = selector.get_support(indices=True)
        selected_features = X.columns[selected_feature_indices]
        X_selected_df = pd.DataFrame(X_selected, columns=selected_features)

        return X_selected_df

    def _remove_highly_correlated_features(self, df, threshold):
        # Step 1: Calculate the correlation matrix
        correlation_matrix = df.corr()

        # Step 2: Find highly correlated features
        highly_correlated = set()
        for i in range(len(correlation_matrix.columns)):
            for j in range(i):
                correlation_value = abs(correlation_matrix.iloc[i, j])
                if correlation_value >= threshold:  # Adjust the threshold as needed
                    colname_i = correlation_matrix.columns[i]
                    colname_j = correlation_matrix.columns[j]
                    if colname_i.lower() != 'virustotal.malicious' and colname_j.lower() != 'virustotal.malicious':
                        highly_correlated.add(colname_i)
                        highly_correlated.add(colname_j)
                    else:
                        pass

        df_filtered = df.drop(columns=highly_correlated)
        return df_filtered
    def _generate_roc_curve(self, clf):
        # Calculate ROC curve and AUC
        fpr, tpr, thresholds = roc_curve(self.y_test, clf.predict_proba(self.X_test)[:, 1])
        auc = roc_auc_score(self.y_test, clf.predict_proba(self.X_test)[:, 1])

        # Append data to lists
        self.roc_curves.append((fpr, tpr))
        self.auc_scores.append(auc)
        self.classifier_names.append(clf)

    def classification_tree(self):

        # 2. Create and train a Decision Tree Classifier on the training data
        clf = DecisionTreeClassifier(random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)


        classifier = 'classification tree'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row

        self._generate_roc_curve(clf)

        #print(new_row)
        # Print or use these metrics as needed
        # print(f'Accuracy: {accuracy:.2f}')
        # print(f'Precision: {precision:.2f}')
        # print(f'Recall: {recall:.2f}')
        # print(f'F1 Score: {f1:.2f}')

    def neural_network(self):

        # 2. Create and train a Multi-Layer Perceptron (Neural Network) Classifier
        clf = MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'neural network'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row
        
        self._generate_roc_curve(clf)

    def naive_bayes(self):

        # 2. Create and train a Gaussian Naive Bayes Classifier
        clf = GaussianNB()
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'naives bayes'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row
        self._generate_roc_curve(clf)

    def knn_classification(self, k=5):

        # 2. Create and train a K-Nearest Neighbors Classifier on the training data
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = knn.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'KNN'
        ## 10 fold cross validation
        cv_scores = cross_val_score(knn,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row
        self._generate_roc_curve(knn)

    def support_vector_machine(self):

        # 2. Create and train an SVM Classifier
        clf = SVC(kernel='linear', random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)

        classifier = 'SVM'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row 

        

        # Wrap it in a CalibratedClassifierCV to enable probability estimates
        clf_calibrated = CalibratedClassifierCV(base_estimator=clf, method='sigmoid')
        clf_calibrated.fit(self.X_train, self.y_train)
        self._generate_roc_curve(clf_calibrated)

    def random_forest(self):

        # 2. Create and train a Random Forest Classifier
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        clf.fit(self.X_train, self.y_train)

        # 3. Use the trained model to make predictions on the test data
        y_pred = clf.predict(self.X_test)

        # 4. Evaluate the model's performance
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred)
        recall = recall_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test,y_pred)

        classifier = 'random forest'
        ## 10 fold cross validation
        cv_scores = cross_val_score(clf,self.X_train , self.y_train, cv=10, scoring='accuracy')
        mean_score = cv_scores.mean()
        std_deviation = cv_scores.std()
        new_row = {'classifier': classifier, 'accuracy':accuracy,'precision':precision,'recall':recall,'f1_score':f1,'cv_avg':mean_score,'cv_std_dev':std_deviation}
        self.results_df.loc[len(self.results_df.index)] = new_row
        self._generate_roc_curve(clf)

    def generate_confusion_matric(self, y_test, y_pred):
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('Predicted')
        plt.ylabel('Actual')
        plt.title('Confusion Matrix')
        plt.show()
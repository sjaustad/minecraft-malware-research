from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import os, time

def extract_usage_data(url):
    cpu_usage = []
    memory_usage = []

    options = Options()
    #options.headless = True

    # Specify the path to the geckodriver executable
    script_directory = os.path.dirname(os.path.abspath(__file__))
   # geckodriver_path = os.path.join(script_directory, 'geckodriver.exe')
    #service = Service(geckodriver_path)
    
    service = Service()
    driver = webdriver.Firefox(service=service, options=options)

    driver.get(url)

    # Wait for at least one CPU element to be present
    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'memory')))

    # Keep track of the number of CPU elements found
    prev_num_cpu_elements = 0

    # Wait until no new CPU elements are loaded for 5 seconds
    while True:
        time.sleep(2)  # Wait for 2 seconds
        cpu_elements = driver.find_elements(By.CLASS_NAME, 'memory')
        num_cpu_elements = len(cpu_elements)

        if num_cpu_elements == prev_num_cpu_elements:
            # No new CPU elements loaded, break the loop
            break

        prev_num_cpu_elements = num_cpu_elements


    # Perform any interactions or actions needed to load the dynamic content, if required
    # For example, scrolling or clicking on elements

    # Extract the page source after loading the dynamic content
    page_source = driver.page_source

    # Close the browser
    driver.quit()

    soup = BeautifulSoup(page_source, 'html.parser')

    cpu_class = 'cpu'
    memory_class = 'memory'
    svg_class = 'peity'

    # Extract CPU usage data
    cpu_elements = soup.find(class_=cpu_class)
    if cpu_elements:
        chart_elements = cpu_elements.find(class_='chart')
        if chart_elements:
            svg_elements = chart_elements.find_all(class_=svg_class)
            for svg in svg_elements:
                rect_elements = svg.find_all('rect')
                for rect in rect_elements:
                    y_value = rect.get('y')
                    if y_value is not None:
                        cpu_usage.append(float(y_value))

    # Extract memory usage data
    memory_elements = soup.find(class_=memory_class)
    if memory_elements:
        chart_elements = memory_elements.find(class_='chart')
        if chart_elements:
            svg_elements = chart_elements.find_all(class_=svg_class)
            for svg in svg_elements:
                rect_elements = svg.find_all('rect')
                for rect in rect_elements:
                    y_value = rect.get('y')
                    if y_value is not None:
                        memory_usage.append(float(y_value))

    return cpu_usage, memory_usage

# # Example URL
# url = 'https://app.any.run/tasks/3609465a-7015-40aa-b5cb-a22bd7bd4254'  # Replace with your desired URL

# # Extract CPU and memory usage data
# cpu_usage, memory_usage = extract_usage_data(url)

# # Print the CPU usage and memory usage lists
# print("CPU Usage:", cpu_usage)
# print("CPU len:", len(cpu_usage))
# print("memory Usage:", memory_usage)
# print("memory len:", len(memory_usage))

import os
import json
from web_cpu_memory_usage import extract_usage_data

def import_json_files(folder_path):
    json_data = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.json'):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                try:
                    data = json.load(file)
                    json_data.append(data)
                except json.JSONDecodeError as e:
                    print(f"Error decoding JSON file {filename}: {e}")
    return json_data

def find_ip_region(ip):
    return None

def build_process_tree(processes, application_name):
    root_process = None
    
    # Find the root process
    for process in processes:
        if process['fileName'] == application_name:
            root_process = process
            break
    
    if root_process is None:
        return None
    
    # Recursively find children processes
    def find_children(parent_process):
        children = []
        
        for process in processes:
            if process['ppid'] == parent_process['pid']:
                children.append(process)
        
        for child in children:
            child['children'] = find_children(child)
        
        return children
    
    root_process['children'] = find_children(root_process)
    return root_process

def find_process_triggers(processes):
    binary_calls = []

    for process in processes:
        #file_name = os.path.basename(process['fileName'])
        binary_calls.append(process['fileName'])

        if 'modules' in process:
            for module in process['modules']:
                if 'image' in module:
                    module_image = os.path.basename(module['image'])
                    binary_calls.append(module_image)

    return binary_calls

def parse_analysis(run):
    data = {}
    
    ## Build process tree
    data['application'] = {
        'name':run['analysis']['content']['mainObject']['filename'],
        'hash':run['analysis']['content']['mainObject']['hashes']['sha256'],
        'duration':run['analysis']['duration']
    }
    data['application']['root_process'] = build_process_tree(run['processes'],data['application']['name'])


    ## network
    data['network'] = {}
    data['network']['counters'] = {
        'dns':run['counters']['network']['dns'],
        'connections':run['counters']['network']['connections'],
        'http':run['counters']['network']['http']
    }
    # aggregate all network traffic
    data['network'] = []
    for entry in run['network']['dnsRequests']:
        for ip in entry['ips']:
            data['network'].append({'ip':ip,'reputation':entry['reputation'],'type':'dns'})
    for entry in run['network']['httpRequests']:
        data['network'].append({'ip':entry['ip'],'reputation':entry['reputation'],'port':entry['port'],'type':'http'})
    for entry in run['network']['connections']:
        data['network'].append({'ip':entry['ip'],'reputation':entry['reputation'], 'port':entry['port'],'type':'connection'})
    
    # get geo region of IPs
    for ip in data['network']:
        ip['region'] = find_ip_region(ip['ip'])

    ## System Calls
    data['sys_calls'] = {
        'files':{},
        'registry':{}
    }
    data['sys_calls']['files']['modified_count'] = len(run['modified']['files'])


    data['sys_calls']['registry']['read'] = run['counters']['registry']['read']
    data['sys_calls']['registry']['write'] = run['counters']['registry']['write']
    data['sys_calls']['registry']['delete'] = run['counters']['registry']['delete']

    data['sys_calls']['binary_triggers'] = find_process_triggers(run['processes'])

    ## memory
    data['memory'] = {
        'dll_list':[]
    }
    for binary in data['sys_calls']['binary_triggers']:
        if ".dll" in binary:
            data['memory']['dll_list'].append(binary)

    ## check for changes to startup files
    data['persistence'] = {
        'suspicious_files':[],
        'suspicious_binaries':[]
    }
    for file_event in run['modified']['files']:
        sus_strings = ['Start Menu','StartUp']
        for string in sus_strings:
            if string.lower() in file_event['filename'].lower():
                data['persistence']['suspicious_files'].append(file_event)
                break
    
    for binary_event in data['sys_calls']['binary_triggers']:
        sus_binaries = ['Reg.exe', 'Nslookup.exe', 'Regasm.exe', 'Runas.exe', 'Schtasks.exe', 'Sc.exe']
        for binary in sus_binaries:
            if binary.lower() == binary_event.lower():
                data['persistence']['suspicious_binaries'].append(binary)

    ## get web information
    url = run['analysis']['permanentUrl']

    data['anomaly_detection']={
        'cpu':{},
        'memory':{}
    }
    cpu_usage, memory_usage = extract_usage_data(url)
    data['anomaly_detection']['cpu']['usage'] = cpu_usage
    data['anomaly_detection']['memory']['usage'] = memory_usage

    return data
    

# Provide the folder path to the "json" folder
source_folder_path = 'json'  # Update this path according to your folder structure
results_folder_path = 'parsed_analyses'

# Call the function and get the list of dictionaries
json_list = import_json_files(source_folder_path)

if not os.path.exists(results_folder_path):
    os.makedirs(results_folder_path)

# Print the imported JSON data
for json_dict in json_list:
    ## analyze file
    data = parse_analysis(json_dict)
    
    ## save file
    file_path = os.path.join(results_folder_path, f"{data['application']['hash']}.json")
    with open(file_path, 'w') as file:
        json.dump(data, file)
